"""
Step 3: Model Fine-Tuning Pipeline

This script handles fine-tuning of InLegalBERT for improved legal retrieval.

Method: Contrastive Learning with TripletLoss
- Training data: Triplets (query, positive, negative)
- Generated by: 07_prepare_training.py

Requirements:
- GPU with 16GB+ VRAM (T4 sufficient, L4/A100 recommended)
- Training triplets in data/processed/training_triplets.jsonl

Usage:
    python pipelines/offline/03_finetune.py [--dry-run] [--epochs 3]

For MVP: Skip fine-tuning and use pre-trained InLegalBERT directly.
"""

import os
import sys
import json
import argparse
from pathlib import Path
from typing import Optional, List, Dict
from loguru import logger

# Configure logging
logger.add(
    "logs/finetune_{time}.log",
    rotation="10 MB",
    level="INFO"
)


def check_environment() -> dict:
    """Check if fine-tuning prerequisites are met"""
    status = {
        "gpu_available": False,
        "gpu_name": None,
        "vram_gb": 0,
        "hf_token": bool(os.getenv("HUGGINGFACE_TOKEN")),
        "torch_installed": False,
        "transformers_installed": False,
    }
    
    # Check PyTorch and GPU
    try:
        import torch
        status["torch_installed"] = True
        status["gpu_available"] = torch.cuda.is_available()
        
        if status["gpu_available"]:
            status["gpu_name"] = torch.cuda.get_device_name(0)
            status["vram_gb"] = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    except ImportError:
        pass
    
    # Check Transformers
    try:
        import transformers
        status["transformers_installed"] = True
    except ImportError:
        pass
    
    return status


def prepare_training_data(
    triplet_file: str = "data/processed/training_triplets.jsonl",
) -> Optional[List[Dict]]:
    """
    Load training triplets prepared by 07_prepare_training.py.
    
    Format: [{"query": ..., "positive": ..., "negative": ...}, ...]
    """
    triplet_path = Path(triplet_file)
    
    if not triplet_path.exists():
        logger.warning(f"Triplet file not found: {triplet_file}")
        logger.info("Run 07_prepare_training.py first to generate triplets.")
        return None
    
    triplets = []
    with open(triplet_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                triplets.append(json.loads(line))
    
    logger.info(f"Loaded {len(triplets)} triplets from {triplet_file}")
    return triplets


def finetune_inlegalbert(
    triplets: List[Dict],
    output_dir: str = "models/inlegalbert-finetuned",
    epochs: int = 3,
    batch_size: int = 16,
    warmup_steps: int = 100,
    learning_rate: float = 2e-5,
    dry_run: bool = False
) -> Optional[str]:
    """
    Fine-tune InLegalBERT for legal retrieval using TripletLoss.
    
    Args:
        triplets: List of training triplets
        output_dir: Directory to save fine-tuned model
        epochs: Number of training epochs
        batch_size: Training batch size
        dry_run: If True, validate setup without training
        
    Returns:
        Path to fine-tuned model or None on failure
    """
    status = check_environment()
    
    if not status["gpu_available"]:
        logger.error("GPU not available. Fine-tuning requires GPU.")
        logger.info("Use Kaggle (free T4) or Lightning AI for GPU access.")
        return None
    
    logger.info(f"GPU: {status['gpu_name']} ({status['vram_gb']:.1f}GB)")
    
    # Adjust batch size for available VRAM
    if status["vram_gb"] < 12:
        batch_size = 8
        logger.warning(f"Low VRAM. Reduced batch size to {batch_size}")
    elif status["vram_gb"] < 20:
        batch_size = 12
        logger.info(f"Moderate VRAM. Using batch size {batch_size}")
    
    try:
        from sentence_transformers import SentenceTransformer, InputExample, losses
        from torch.utils.data import DataLoader
    except ImportError:
        logger.error("sentence-transformers not installed. Run: pip install sentence-transformers")
        return None
    
    # Load pre-trained InLegalBERT
    logger.info("Loading InLegalBERT base model...")
    model = SentenceTransformer("law-ai/InLegalBERT")
    
    # Create training examples
    logger.info("Creating training examples...")
    train_examples = []
    for t in triplets:
        example = InputExample(
            texts=[t["query"], t["positive"], t["negative"]]
        )
        train_examples.append(example)
    
    logger.info(f"Training examples: {len(train_examples)}")
    
    if dry_run:
        logger.info("Dry run: Validation complete. Exiting before training.")
        return None
    
    # Create DataLoader
    train_dataloader = DataLoader(
        train_examples, 
        shuffle=True, 
        batch_size=batch_size
    )
    
    # TripletLoss for contrastive learning
    train_loss = losses.TripletLoss(
        model=model,
        distance_metric=losses.TripletDistanceMetric.COSINE,
        triplet_margin=0.5
    )
    
    # Calculate warmup steps
    total_steps = len(train_dataloader) * epochs
    warmup_steps = min(warmup_steps, int(total_steps * 0.1))
    
    logger.info(f"Starting fine-tuning...")
    logger.info(f"  Epochs: {epochs}")
    logger.info(f"  Batch size: {batch_size}")
    logger.info(f"  Total steps: {total_steps}")
    logger.info(f"  Warmup steps: {warmup_steps}")
    
    # Create output directory
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Fine-tune
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=epochs,
        warmup_steps=warmup_steps,
        output_path=str(output_path),
        show_progress_bar=True
    )
    
    logger.info(f"âœ… Fine-tuning complete. Model saved to {output_path}")
    return str(output_path)


def quantize_model(
    model_path: str,
    output_path: str,
    bits: int = 4
) -> Optional[str]:
    """
    Quantize model to reduce memory footprint.
    
    4-bit quantization reduces size by ~8x with minimal accuracy loss.
    """
    logger.warning("Model quantization not implemented yet")
    return None


def main():
    """Main entry point for fine-tuning pipeline"""
    parser = argparse.ArgumentParser(description="Fine-tune InLegalBERT")
    parser.add_argument("--dry-run", action="store_true", help="Validate setup without training")
    parser.add_argument("--epochs", type=int, default=3, help="Training epochs")
    parser.add_argument("--batch-size", type=int, default=16, help="Batch size")
    parser.add_argument("--triplet-file", type=str, default="data/processed/training_triplets.jsonl")
    parser.add_argument("--output-dir", type=str, default="models/inlegalbert-finetuned")
    
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print("NEETHI APP - FINE-TUNING PIPELINE (Step 3)")
    print("="*60 + "\n")
    
    # Check environment
    print("Checking environment...")
    status = check_environment()
    
    print(f"\n{'Component':<25} {'Status':<15}")
    print("-" * 40)
    print(f"{'PyTorch':<25} {'âœ… Installed' if status['torch_installed'] else 'âŒ Missing'}")
    print(f"{'Transformers':<25} {'âœ… Installed' if status['transformers_installed'] else 'âŒ Missing'}")
    print(f"{'GPU Available':<25} {'âœ… Yes' if status['gpu_available'] else 'âŒ No'}")
    if status['gpu_available']:
        print(f"{'GPU Name':<25} {status['gpu_name']}")
        print(f"{'VRAM':<25} {status['vram_gb']:.1f} GB")
    print(f"{'HuggingFace Token':<25} {'âœ… Set' if status['hf_token'] else 'âš ï¸ Not set (optional)'}")
    print()
    
    if not status['gpu_available']:
        print("âš ï¸  GPU not available. Fine-tuning skipped.")
        print("\nFor MVP, use pre-trained models directly:")
        print("  Embedding: law-ai/InLegalBERT")
        print("  Generation: Llama-3 via Groq API")
        print("\nTo fine-tune later:")
        print("  1. Use Kaggle (free T4 GPU) or Lightning AI")
        print("  2. Generate training data: python 07_prepare_training.py")
        print("  3. Run: python 03_finetune.py --epochs 3")
        return
    
    # Load training data
    triplets = prepare_training_data(args.triplet_file)
    
    if not triplets:
        print("\nâš ï¸  Training data not available.")
        print(f"Expected: {args.triplet_file}")
        print("\nGenerate training data first:")
        print("  1. python 06_download_datasets.py")
        print("  2. python 07_prepare_training.py")
        return
    
    print(f"\nðŸ“Š Training data: {len(triplets)} triplets")
    
    if args.dry_run:
        print("\nðŸ” Dry run mode - validating setup...")
    
    # Fine-tune embedding model
    model_path = finetune_inlegalbert(
        triplets,
        output_dir=args.output_dir,
        epochs=args.epochs,
        batch_size=args.batch_size,
        dry_run=args.dry_run
    )
    
    if model_path:
        print(f"\nâœ… Fine-tuned model saved to: {model_path}")
        print("\nNext step: Run 04_populate_qdrant.py to update vector database")
    elif args.dry_run:
        print("\nâœ… Dry run complete. Setup is valid.")
    else:
        print("\nâš ï¸  Fine-tuning failed or skipped.")
        print("Using pre-trained InLegalBERT for MVP.")


if __name__ == "__main__":
    main()

