{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Neethi App - Offline Data Pipeline\n",
                "\n",
                "This notebook combines all offline data preparation scripts into a single executable workflow for injecting data into Qdrant.\n",
                "\n",
                "## Pipeline Steps\n",
                "1. **Download Datasets** - Download legal datasets from HuggingFace\n",
                "2. **Data Cleaning** - Clean and standardize IPC/BNS/BSA data\n",
                "3. **Semantic Chunking** - Create embeddings-ready chunks\n",
                "4. **Prepare Training Data** - Generate triplets for fine-tuning (optional)\n",
                "5. **Fine-tune Model** - Fine-tune InLegalBERT (requires GPU)\n",
                "6. **Populate Qdrant** - Index statute chunks into Qdrant\n",
                "7. **Ingest Bail Judgments** - Download and index bail judgments\n",
                "8. **Extract SC Judgments** - Extract 26K SC judgments from PDFs\n",
                "9. **Ingest SC Judgments** - Index SC judgments into Qdrant\n",
                "\n",
                "## Prerequisites\n",
                "- Qdrant Cloud account with QDRANT_URL and QDRANT_API_KEY in .env\n",
                "- Required packages: `qdrant-client`, `sentence-transformers`, `datasets`, `loguru`, `pandas`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup & Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (uncomment if needed)\n",
                "# !pip install qdrant-client sentence-transformers datasets loguru pandas python-dotenv pdfplumber kagglehub huggingface_hub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import re\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "from typing import Dict, List, Optional, Generator, Any, Tuple\n",
                "from dataclasses import dataclass, field, asdict\n",
                "from dotenv import load_dotenv\n",
                "from loguru import logger\n",
                "\n",
                "# Load environment variables\n",
                "load_dotenv()\n",
                "\n",
                "# Configure logging\n",
                "logger.add(\"logs/offline_pipeline_{time}.log\", rotation=\"10 MB\", level=\"INFO\")\n",
                "\n",
                "# Create directories\n",
                "Path(\"data/raw\").mkdir(parents=True, exist_ok=True)\n",
                "Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n",
                "Path(\"data/training\").mkdir(parents=True, exist_ok=True)\n",
                "Path(\"logs\").mkdir(exist_ok=True)\n",
                "\n",
                "print(\"‚úÖ Setup complete\")\n",
                "print(f\"QDRANT_URL: {os.getenv('QDRANT_URL', 'NOT SET')[:50]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Download Datasets from HuggingFace"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def download_aalap(output_dir: Path, max_samples: Optional[int] = None) -> Dict[str, int]:\n",
                "    \"\"\"Download Aalap instruction dataset from HuggingFace\"\"\"\n",
                "    from datasets import load_dataset\n",
                "    \n",
                "    logger.info(\"Downloading Aalap instruction dataset...\")\n",
                "    \n",
                "    try:\n",
                "        ds = load_dataset(\"opennyaiorg/aalap_instruction_dataset\", split=\"train\")\n",
                "        logger.info(f\"Aalap columns: {ds.column_names}\")\n",
                "        \n",
                "        task_counts = {}\n",
                "        all_examples = []\n",
                "        \n",
                "        for i, example in enumerate(ds):\n",
                "            if max_samples and i >= max_samples:\n",
                "                break\n",
                "            \n",
                "            normalized = dict(example)\n",
                "            normalized[\"id\"] = f\"aalap_{i}\"\n",
                "            normalized[\"source\"] = \"aalap\"\n",
                "            \n",
                "            task = example.get(\"task_type\") or example.get(\"task\") or \"instruction\"\n",
                "            normalized[\"task\"] = task\n",
                "            task_counts[task] = task_counts.get(task, 0) + 1\n",
                "            all_examples.append(normalized)\n",
                "        \n",
                "        output_path = output_dir / \"aalap_train.jsonl\"\n",
                "        with open(output_path, 'w', encoding='utf-8') as f:\n",
                "            for ex in all_examples:\n",
                "                f.write(json.dumps(ex, ensure_ascii=False) + '\\n')\n",
                "        \n",
                "        logger.info(f\"‚úÖ Aalap saved: {len(all_examples)} examples\")\n",
                "        return task_counts\n",
                "    except Exception as e:\n",
                "        logger.error(f\"‚ùå Failed: {e}\")\n",
                "        return {}\n",
                "\n",
                "def download_legal_texts(output_dir: Path, max_samples: Optional[int] = None) -> int:\n",
                "    \"\"\"Download Indian legal texts dataset\"\"\"\n",
                "    from huggingface_hub import hf_hub_download\n",
                "    \n",
                "    logger.info(\"Downloading Indian legal texts...\")\n",
                "    \n",
                "    repo_id = \"Techmaestro369/indian-legal-texts-finetuning\"\n",
                "    files = [\"ipc_qa.json\", \"crpc_qa.json\", \"constitution_qa.json\"]\n",
                "    all_examples = []\n",
                "    \n",
                "    for filename in files:\n",
                "        try:\n",
                "            file_path = hf_hub_download(repo_id=repo_id, filename=filename, repo_type=\"dataset\")\n",
                "            with open(file_path, 'r', encoding='utf-8') as f:\n",
                "                data = json.load(f)\n",
                "            \n",
                "            source = filename.replace(\"_qa.json\", \"\").upper()\n",
                "            for i, item in enumerate(data):\n",
                "                if max_samples and len(all_examples) >= max_samples:\n",
                "                    break\n",
                "                normalized = {\n",
                "                    \"id\": f\"legal_texts_{source}_{i}\",\n",
                "                    \"question\": item.get(\"question\", \"\"),\n",
                "                    \"answer\": item.get(\"answer\", \"\"),\n",
                "                    \"source\": source,\n",
                "                    \"section\": item.get(\"section\", \"\")\n",
                "                }\n",
                "                all_examples.append(normalized)\n",
                "            logger.info(f\"  ‚úÖ {filename}: {len(data)} examples\")\n",
                "        except Exception as e:\n",
                "            logger.warning(f\"  ‚ö†Ô∏è Failed {filename}: {e}\")\n",
                "    \n",
                "    output_path = output_dir / \"legal_texts.jsonl\"\n",
                "    with open(output_path, 'w', encoding='utf-8') as f:\n",
                "        for ex in all_examples:\n",
                "            f.write(json.dumps(ex, ensure_ascii=False) + '\\n')\n",
                "    \n",
                "    logger.info(f\"‚úÖ Legal texts saved: {len(all_examples)} examples\")\n",
                "    return len(all_examples)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run dataset download\n",
                "output_dir = Path(\"data/training\")\n",
                "\n",
                "print(\"üì• Downloading datasets...\")\n",
                "task_counts = download_aalap(output_dir)\n",
                "legal_count = download_legal_texts(output_dir)\n",
                "\n",
                "print(f\"\\nüìä Download Summary:\")\n",
                "print(f\"   Aalap tasks: {task_counts}\")\n",
                "print(f\"   Legal texts: {legal_count}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Data Cleaning (IPC/BNS/BSA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LegalDataCleaner:\n",
                "    \"\"\"Cleans and standardizes legal datasets for RAG ingestion.\"\"\"\n",
                "    \n",
                "    def __init__(self, raw_dir: str = \"data/raw\", processed_dir: str = \"data/processed\"):\n",
                "        self.raw_dir = Path(raw_dir)\n",
                "        self.processed_dir = Path(processed_dir)\n",
                "        self.processed_dir.mkdir(parents=True, exist_ok=True)\n",
                "        self.stats = {\"total\": 0, \"cleaned\": 0, \"skipped\": 0}\n",
                "    \n",
                "    def clean_ipc_dataset(self, filename: str = \"ipc_sections.csv\") -> str:\n",
                "        \"\"\"Clean IPC sections dataset\"\"\"\n",
                "        filepath = self.raw_dir / filename\n",
                "        if not filepath.exists():\n",
                "            logger.warning(f\"File not found: {filepath}\")\n",
                "            return \"\"\n",
                "        \n",
                "        df = pd.read_csv(filepath, encoding='utf-8')\n",
                "        df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
                "        \n",
                "        cleaned_records = []\n",
                "        for idx, row in df.iterrows():\n",
                "            section_num = str(row.get('section', row.get('section_num', ''))).strip()\n",
                "            if not section_num or section_num.lower() == 'nan':\n",
                "                continue\n",
                "            \n",
                "            section_num = re.sub(r'[^\\dA-Za-z]', '', section_num)\n",
                "            description = str(row.get('description', '')).strip()\n",
                "            offense = str(row.get('offense', '')).strip()\n",
                "            punishment = str(row.get('punishment', '')).strip()\n",
                "            \n",
                "            text = f\"IPC Section {section_num}: {description}\"\n",
                "            if offense and offense.lower() != 'nan':\n",
                "                text += f\". Offense: {offense}\"\n",
                "            if punishment and punishment.lower() != 'nan':\n",
                "                text += f\". Punishment: {punishment}\"\n",
                "            \n",
                "            cleaned_records.append({\n",
                "                \"id\": f\"IPC_{section_num}\",\n",
                "                \"section_num\": section_num,\n",
                "                \"law_type\": \"IPC\",\n",
                "                \"description\": description,\n",
                "                \"offense\": offense,\n",
                "                \"punishment\": punishment,\n",
                "                \"text\": text\n",
                "            })\n",
                "        \n",
                "        output_path = self.processed_dir / \"ipc_clean.jsonl\"\n",
                "        with open(output_path, 'w', encoding='utf-8') as f:\n",
                "            for rec in cleaned_records:\n",
                "                f.write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
                "        \n",
                "        self.stats[\"cleaned\"] += len(cleaned_records)\n",
                "        logger.info(f\"‚úÖ Cleaned {len(cleaned_records)} IPC sections\")\n",
                "        return str(output_path)\n",
                "    \n",
                "    def clean_bns_dataset(self, filename: str = \"bns_sections.csv\") -> str:\n",
                "        \"\"\"Clean BNS sections dataset\"\"\"\n",
                "        filepath = self.raw_dir / filename\n",
                "        if not filepath.exists():\n",
                "            logger.warning(f\"File not found: {filepath}\")\n",
                "            return \"\"\n",
                "        \n",
                "        df = pd.read_csv(filepath, encoding='utf-8')\n",
                "        df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
                "        \n",
                "        cleaned_records = []\n",
                "        for idx, row in df.iterrows():\n",
                "            section_num = str(row.get('section', '')).strip()\n",
                "            if not section_num or section_num.lower() == 'nan':\n",
                "                continue\n",
                "            \n",
                "            section_num = re.sub(r'[^\\dA-Za-z]', '', section_num)\n",
                "            section_name = str(row.get('section_name', '')).strip()\n",
                "            description = str(row.get('description', section_name)).strip()\n",
                "            \n",
                "            text = f\"BNS Section {section_num}\"\n",
                "            if section_name and section_name.lower() != 'nan':\n",
                "                text += f\" ({section_name})\"\n",
                "            text += f\": {description}\"\n",
                "            \n",
                "            cleaned_records.append({\n",
                "                \"id\": f\"BNS_{section_num}\",\n",
                "                \"section_num\": section_num,\n",
                "                \"law_type\": \"BNS\",\n",
                "                \"section_name\": section_name,\n",
                "                \"description\": description,\n",
                "                \"text\": text\n",
                "            })\n",
                "        \n",
                "        output_path = self.processed_dir / \"bns_clean.jsonl\"\n",
                "        with open(output_path, 'w', encoding='utf-8') as f:\n",
                "            for rec in cleaned_records:\n",
                "                f.write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
                "        \n",
                "        self.stats[\"cleaned\"] += len(cleaned_records)\n",
                "        logger.info(f\"‚úÖ Cleaned {len(cleaned_records)} BNS sections\")\n",
                "        return str(output_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run data cleaning (requires CSV files in data/raw/)\n",
                "cleaner = LegalDataCleaner()\n",
                "\n",
                "print(\"üßπ Cleaning datasets...\")\n",
                "ipc_output = cleaner.clean_ipc_dataset()\n",
                "bns_output = cleaner.clean_bns_dataset()\n",
                "\n",
                "print(f\"\\nüìä Cleaning Summary:\")\n",
                "print(f\"   Total cleaned: {cleaner.stats['cleaned']}\")\n",
                "if ipc_output:\n",
                "    print(f\"   IPC: {ipc_output}\")\n",
                "if bns_output:\n",
                "    print(f\"   BNS: {bns_output}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Semantic Chunking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class Chunk:\n",
                "    \"\"\"A single chunk ready for embedding\"\"\"\n",
                "    chunk_id: str\n",
                "    text: str\n",
                "    law_type: str\n",
                "    section_num: Optional[str] = None\n",
                "    metadata: Dict = field(default_factory=dict)\n",
                "    \n",
                "    def to_dict(self) -> Dict:\n",
                "        return asdict(self)\n",
                "\n",
                "class SemanticChunker:\n",
                "    \"\"\"Creates chunks from legal documents\"\"\"\n",
                "    \n",
                "    def __init__(self, processed_dir: str = \"data/processed\"):\n",
                "        self.processed_dir = Path(processed_dir)\n",
                "        self.stats = {\"chunks\": 0, \"by_type\": {}}\n",
                "    \n",
                "    def chunk_statutes(self) -> Generator[Chunk, None, None]:\n",
                "        \"\"\"Process statute files\"\"\"\n",
                "        for filename in [\"ipc_clean.jsonl\", \"bns_clean.jsonl\"]:\n",
                "            filepath = self.processed_dir / filename\n",
                "            if not filepath.exists():\n",
                "                continue\n",
                "            \n",
                "            with open(filepath, 'r', encoding='utf-8') as f:\n",
                "                for line in f:\n",
                "                    record = json.loads(line.strip())\n",
                "                    text = record.get(\"text\", \"\")\n",
                "                    if len(text) < 20:\n",
                "                        continue\n",
                "                    \n",
                "                    law_type = record.get(\"law_type\", \"UNKNOWN\")\n",
                "                    section_num = record.get(\"section_num\", \"\")\n",
                "                    \n",
                "                    chunk = Chunk(\n",
                "                        chunk_id=f\"{law_type}_{section_num}\",\n",
                "                        text=text,\n",
                "                        law_type=law_type,\n",
                "                        section_num=section_num,\n",
                "                        metadata={\"offense\": record.get(\"offense\", \"\"), \"punishment\": record.get(\"punishment\", \"\")}\n",
                "                    )\n",
                "                    self.stats[\"chunks\"] += 1\n",
                "                    self.stats[\"by_type\"][law_type] = self.stats[\"by_type\"].get(law_type, 0) + 1\n",
                "                    yield chunk\n",
                "    \n",
                "    def run(self, output_file: str = \"chunks.jsonl\") -> str:\n",
                "        \"\"\"Run chunking pipeline\"\"\"\n",
                "        output_path = self.processed_dir / output_file\n",
                "        \n",
                "        with open(output_path, 'w', encoding='utf-8') as f:\n",
                "            for chunk in self.chunk_statutes():\n",
                "                f.write(json.dumps(chunk.to_dict(), ensure_ascii=False) + '\\n')\n",
                "        \n",
                "        logger.info(f\"‚úÖ Created {self.stats['chunks']} chunks\")\n",
                "        return str(output_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run chunking\n",
                "chunker = SemanticChunker()\n",
                "output_path = chunker.run()\n",
                "\n",
                "print(f\"\\nüìä Chunking Summary:\")\n",
                "print(f\"   Total chunks: {chunker.stats['chunks']}\")\n",
                "print(f\"   By type: {chunker.stats['by_type']}\")\n",
                "print(f\"   Output: {output_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Populate Qdrant with Statute Chunks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from qdrant_client import QdrantClient\n",
                "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "def populate_qdrant(\n",
                "    chunks_file: str = \"data/processed/chunks.jsonl\",\n",
                "    collection_name: str = \"neethi-legal-kb\",\n",
                "    embedding_model: str = \"law-ai/InLegalBERT\",\n",
                "    batch_size: int = 32,\n",
                "    recreate: bool = False\n",
                "):\n",
                "    \"\"\"Embed chunks and index into Qdrant\"\"\"\n",
                "    \n",
                "    # Connect to Qdrant\n",
                "    qdrant_url = os.getenv(\"QDRANT_URL\")\n",
                "    qdrant_key = os.getenv(\"QDRANT_API_KEY\")\n",
                "    \n",
                "    if not qdrant_url:\n",
                "        raise ValueError(\"QDRANT_URL not set!\")\n",
                "    \n",
                "    client = QdrantClient(url=qdrant_url, api_key=qdrant_key, timeout=60)\n",
                "    logger.info(\"‚úÖ Connected to Qdrant\")\n",
                "    \n",
                "    # Load embedder\n",
                "    logger.info(f\"Loading {embedding_model}...\")\n",
                "    embedder = SentenceTransformer(embedding_model)\n",
                "    vector_size = embedder.get_sentence_embedding_dimension()\n",
                "    logger.info(f\"‚úÖ Embedder loaded (dim={vector_size})\")\n",
                "    \n",
                "    # Create collection\n",
                "    collections = [c.name for c in client.get_collections().collections]\n",
                "    if collection_name in collections:\n",
                "        if recreate:\n",
                "            client.delete_collection(collection_name)\n",
                "            logger.warning(f\"Deleted existing collection: {collection_name}\")\n",
                "        else:\n",
                "            logger.info(f\"Collection exists: {collection_name}\")\n",
                "    \n",
                "    if collection_name not in collections or recreate:\n",
                "        client.create_collection(\n",
                "            collection_name=collection_name,\n",
                "            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
                "        )\n",
                "        logger.info(f\"‚úÖ Created collection: {collection_name}\")\n",
                "    \n",
                "    # Load chunks\n",
                "    chunks = []\n",
                "    with open(chunks_file, 'r', encoding='utf-8') as f:\n",
                "        for line in f:\n",
                "            chunks.append(json.loads(line.strip()))\n",
                "    logger.info(f\"Loaded {len(chunks)} chunks\")\n",
                "    \n",
                "    # Embed and index\n",
                "    texts = [c.get(\"text\", \"\") for c in chunks]\n",
                "    logger.info(\"Generating embeddings...\")\n",
                "    embeddings = embedder.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
                "    \n",
                "    # Create points\n",
                "    points = []\n",
                "    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
                "        payload = {\n",
                "            \"chunk_id\": chunk.get(\"chunk_id\", f\"chunk_{i}\"),\n",
                "            \"text\": chunk.get(\"text\", \"\"),\n",
                "            \"law_type\": chunk.get(\"law_type\", \"\"),\n",
                "            \"section_num\": chunk.get(\"section_num\"),\n",
                "        }\n",
                "        points.append(PointStruct(id=i+1, vector=embedding.tolist(), payload=payload))\n",
                "    \n",
                "    # Upsert in batches\n",
                "    for i in range(0, len(points), batch_size):\n",
                "        batch = points[i:i+batch_size]\n",
                "        client.upsert(collection_name=collection_name, points=batch)\n",
                "    \n",
                "    info = client.get_collection(collection_name)\n",
                "    logger.info(f\"‚úÖ Indexed {len(points)} chunks. Collection has {info.points_count} vectors.\")\n",
                "    \n",
                "    return len(points)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run Qdrant population\n",
                "indexed = populate_qdrant(recreate=False)\n",
                "print(f\"\\n‚úÖ Indexed {indexed} statute chunks into Qdrant!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Ingest Bail Judgments"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ingest_bail_judgments(collection_name: str = \"neethi-bail-judgments\", batch_size: int = 50):\n",
                "    \"\"\"Download and index bail judgments from HuggingFace\"\"\"\n",
                "    from datasets import load_dataset\n",
                "    from qdrant_client import QdrantClient\n",
                "    from qdrant_client.models import VectorParams, Distance, PointStruct\n",
                "    from sentence_transformers import SentenceTransformer\n",
                "    \n",
                "    # Download dataset\n",
                "    logger.info(\"Downloading IndianBailJudgments-1200...\")\n",
                "    dataset = load_dataset(\"SnehaDeshmukh/IndianBailJudgments-1200\")\n",
                "    cases = list(dataset[\"train\"])\n",
                "    logger.info(f\"‚úÖ Downloaded {len(cases)} bail judgments\")\n",
                "    \n",
                "    # Initialize clients\n",
                "    client = QdrantClient(url=os.getenv(\"QDRANT_URL\"), api_key=os.getenv(\"QDRANT_API_KEY\"))\n",
                "    embedder = SentenceTransformer(\"law-ai/InLegalBERT\")\n",
                "    \n",
                "    # Create collection\n",
                "    collections = [c.name for c in client.get_collections().collections]\n",
                "    if collection_name in collections:\n",
                "        client.delete_collection(collection_name)\n",
                "    \n",
                "    client.create_collection(\n",
                "        collection_name=collection_name,\n",
                "        vectors_config=VectorParams(size=768, distance=Distance.COSINE)\n",
                "    )\n",
                "    logger.info(f\"‚úÖ Created collection: {collection_name}\")\n",
                "    \n",
                "    # Process and index\n",
                "    points = []\n",
                "    for i, case in enumerate(cases):\n",
                "        bail_outcome = case.get(\"bail_outcome\", case.get(\"Bail Outcome\", \"Unknown\"))\n",
                "        ipc_raw = case.get(\"ipc_sections\", case.get(\"IPC Sections\", \"\"))\n",
                "        ipc_sections = [s.strip() for s in str(ipc_raw).split(\",\") if s.strip()] if ipc_raw else []\n",
                "        summary = case.get(\"summary\", case.get(\"Summary\", \"\"))\n",
                "        \n",
                "        text_for_embed = f\"Bail {bail_outcome}. IPC: {', '.join(ipc_sections)}. {summary[:1000]}\"\n",
                "        embedding = embedder.encode(text_for_embed).tolist()\n",
                "        \n",
                "        payload = {\n",
                "            \"case_id\": str(case.get(\"id\", f\"bail_{i}\")),\n",
                "            \"bail_outcome\": bail_outcome,\n",
                "            \"ipc_sections\": ipc_sections,\n",
                "            \"summary\": summary,\n",
                "            \"law_type\": \"Bail Judgment\",\n",
                "            \"section_num\": f\"Case-{i}\"\n",
                "        }\n",
                "        points.append(PointStruct(id=i, vector=embedding, payload=payload))\n",
                "        \n",
                "        if len(points) >= batch_size:\n",
                "            client.upsert(collection_name=collection_name, points=points)\n",
                "            logger.info(f\"Indexed {i+1}/{len(cases)}\")\n",
                "            points = []\n",
                "    \n",
                "    if points:\n",
                "        client.upsert(collection_name=collection_name, points=points)\n",
                "    \n",
                "    logger.info(f\"‚úÖ Indexed {len(cases)} bail judgments\")\n",
                "    return len(cases)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run bail judgments ingestion\n",
                "bail_count = ingest_bail_judgments()\n",
                "print(f\"\\n‚úÖ Indexed {bail_count} bail judgments into Qdrant!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Extract SC Judgments 26K (requires Kaggle)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_sc_judgments(limit: Optional[int] = None):\n",
                "    \"\"\"Extract SC judgments from Kaggle PDFs\"\"\"\n",
                "    import kagglehub\n",
                "    import pdfplumber\n",
                "    import glob\n",
                "    from multiprocessing import Pool, cpu_count\n",
                "    \n",
                "    logger.info(\"Downloading SC judgments from Kaggle...\")\n",
                "    cache_path = kagglehub.dataset_download(\"adarshsingh0903/legal-dataset-sc-judgments-india-19502024\")\n",
                "    logger.info(f\"Dataset path: {cache_path}\")\n",
                "    \n",
                "    all_pdfs = glob.glob(f\"{cache_path}/**/*.pdf\", recursive=True)\n",
                "    all_pdfs += glob.glob(f\"{cache_path}/**/*.PDF\", recursive=True)\n",
                "    all_pdfs = list(set(all_pdfs))\n",
                "    logger.info(f\"Found {len(all_pdfs)} PDFs\")\n",
                "    \n",
                "    if limit:\n",
                "        all_pdfs = all_pdfs[:limit]\n",
                "    \n",
                "    output_file = Path(\"data/processed/sc_judgments_26k.jsonl\")\n",
                "    \n",
                "    def extract_pdf(pdf_path):\n",
                "        try:\n",
                "            filename = os.path.basename(pdf_path)\n",
                "            case_id = filename.replace(\".pdf\", \"\").replace(\".PDF\", \"\")\n",
                "            \n",
                "            with pdfplumber.open(pdf_path) as pdf:\n",
                "                full_text = \"\"\n",
                "                for page in pdf.pages:\n",
                "                    text = page.extract_text()\n",
                "                    if text:\n",
                "                        full_text += text + \"\\n\"\n",
                "            \n",
                "            if len(full_text) < 100:\n",
                "                return None\n",
                "            \n",
                "            # Extract URL\n",
                "            url_match = re.search(r'indiankanoon\\.org/doc/(\\d+)', full_text)\n",
                "            doc_url = f\"https://indiankanoon.org/doc/{url_match.group(1)}/\" if url_match else \"\"\n",
                "            \n",
                "            # Chunk text\n",
                "            chunks = []\n",
                "            words = full_text.split()\n",
                "            for i in range(0, len(words), 400):\n",
                "                chunk_text = \" \".join(words[i:i+400])\n",
                "                chunks.append({\n",
                "                    \"case_id\": case_id,\n",
                "                    \"filename\": filename,\n",
                "                    \"doc_url\": doc_url,\n",
                "                    \"chunk_idx\": i // 400,\n",
                "                    \"text\": chunk_text,\n",
                "                    \"year\": \"Unknown\"\n",
                "                })\n",
                "            return chunks\n",
                "        except Exception as e:\n",
                "            return None\n",
                "    \n",
                "    total_chunks = 0\n",
                "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
                "        for i, pdf_path in enumerate(all_pdfs):\n",
                "            chunks = extract_pdf(pdf_path)\n",
                "            if chunks:\n",
                "                for chunk in chunks:\n",
                "                    f.write(json.dumps(chunk) + \"\\n\")\n",
                "                    total_chunks += 1\n",
                "            if (i + 1) % 100 == 0:\n",
                "                logger.info(f\"Processed {i+1}/{len(all_pdfs)} PDFs, {total_chunks} chunks\")\n",
                "    \n",
                "    logger.info(f\"‚úÖ Extracted {total_chunks} chunks to {output_file}\")\n",
                "    return total_chunks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run SC extraction (set limit for testing)\n",
                "# sc_chunks = extract_sc_judgments(limit=100)  # Uncomment to run\n",
                "print(\"‚è∏Ô∏è SC extraction skipped (uncomment to run)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Ingest SC Judgments into Qdrant"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ingest_sc_judgments(\n",
                "    input_file: str = \"data/processed/sc_judgments_26k.jsonl\",\n",
                "    collection_name: str = \"neethi-judgments\",\n",
                "    batch_size: int = 100\n",
                "):\n",
                "    \"\"\"Ingest SC judgment chunks into Qdrant\"\"\"\n",
                "    from qdrant_client import QdrantClient\n",
                "    from qdrant_client.models import VectorParams, Distance, PointStruct\n",
                "    from sentence_transformers import SentenceTransformer\n",
                "    import uuid\n",
                "    \n",
                "    if not Path(input_file).exists():\n",
                "        logger.error(f\"Input file not found: {input_file}\")\n",
                "        return 0\n",
                "    \n",
                "    # Count chunks\n",
                "    total_chunks = sum(1 for _ in open(input_file, encoding=\"utf-8\"))\n",
                "    logger.info(f\"Total chunks to process: {total_chunks}\")\n",
                "    \n",
                "    # Initialize\n",
                "    client = QdrantClient(url=os.getenv(\"QDRANT_URL\"), api_key=os.getenv(\"QDRANT_API_KEY\"), timeout=120)\n",
                "    embedder = SentenceTransformer(\"law-ai/InLegalBERT\")\n",
                "    \n",
                "    # Check/create collection\n",
                "    collections = [c.name for c in client.get_collections().collections]\n",
                "    if collection_name not in collections:\n",
                "        client.create_collection(\n",
                "            collection_name=collection_name,\n",
                "            vectors_config=VectorParams(size=768, distance=Distance.COSINE)\n",
                "        )\n",
                "        logger.info(f\"‚úÖ Created collection: {collection_name}\")\n",
                "    \n",
                "    # Process in batches\n",
                "    indexed = 0\n",
                "    batch_texts, batch_payloads = [], []\n",
                "    \n",
                "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
                "        for line in f:\n",
                "            chunk = json.loads(line)\n",
                "            batch_texts.append(chunk[\"text\"])\n",
                "            batch_payloads.append({\n",
                "                \"case_id\": chunk[\"case_id\"],\n",
                "                \"doc_url\": chunk.get(\"doc_url\", \"\"),\n",
                "                \"text\": chunk[\"text\"],\n",
                "                \"law_type\": \"SC Judgment\",\n",
                "                \"section_num\": chunk[\"case_id\"]\n",
                "            })\n",
                "            \n",
                "            if len(batch_texts) >= batch_size:\n",
                "                embeddings = embedder.encode(batch_texts, show_progress_bar=False)\n",
                "                points = [\n",
                "                    PointStruct(id=str(uuid.uuid4()), vector=emb.tolist(), payload=pay)\n",
                "                    for emb, pay in zip(embeddings, batch_payloads)\n",
                "                ]\n",
                "                client.upsert(collection_name=collection_name, points=points)\n",
                "                indexed += len(points)\n",
                "                \n",
                "                if indexed % 5000 == 0:\n",
                "                    logger.info(f\"Indexed: {indexed}/{total_chunks}\")\n",
                "                \n",
                "                batch_texts, batch_payloads = [], []\n",
                "    \n",
                "    # Final batch\n",
                "    if batch_texts:\n",
                "        embeddings = embedder.encode(batch_texts, show_progress_bar=False)\n",
                "        points = [\n",
                "            PointStruct(id=str(uuid.uuid4()), vector=emb.tolist(), payload=pay)\n",
                "            for emb, pay in zip(embeddings, batch_payloads)\n",
                "        ]\n",
                "        client.upsert(collection_name=collection_name, points=points)\n",
                "        indexed += len(points)\n",
                "    \n",
                "    logger.info(f\"‚úÖ Indexed {indexed} SC judgment chunks\")\n",
                "    return indexed"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run SC ingestion\n",
                "# sc_indexed = ingest_sc_judgments()  # Uncomment to run\n",
                "print(\"‚è∏Ô∏è SC ingestion skipped (uncomment to run)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "This notebook provides a complete offline data pipeline for the Neethi App:\n",
                "\n",
                "1. **Dataset Download** - Aalap instruction data + Legal texts from HuggingFace\n",
                "2. **Data Cleaning** - IPC/BNS sections standardization\n",
                "3. **Chunking** - Create embedding-ready chunks\n",
                "4. **Qdrant Population** - Index statute chunks\n",
                "5. **Bail Judgments** - Download and index 1,200 bail cases\n",
                "6. **SC Judgments** - Extract and index 26K Supreme Court judgments\n",
                "\n",
                "### Collections Created\n",
                "- `neethi-legal-kb` - Statute sections (IPC/BNS)\n",
                "- `neethi-bail-judgments` - Bail judgment cases\n",
                "- `neethi-judgments` - SC judgments"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
